# 新的论文模版放进去

# 添加例子进去

# 在训练集和测试集上面应该说明为什么使用8:1:1

# 量化数据

- [ ] 量化第 6.1 节计算开销（训练/推理成本、时延、调用次数等），给出可复现实验数字。
  - 训练：记录教师模型生成成本（调用次数、平均/总 token、总费用或 GPU 时长）、SFT 微调开销（epoch、batch、硬件、时长）。
  - 推理：单样本平均时延、平均 token、并发设置、稳定性（超时/失败率）。
  - 以统一单位呈现成本（如 USD/万样本、GPU-hour/epoch）并提供估算脚本或表格。

  针对数据量是10万条，调用次数10万次，平均token 2000左右，中token 2,000*100,000=200,000,000 token
总gpu时长在Nvidia 5090 * 2 时长是38h

微调开销是8个h
epoch2
batch 16
